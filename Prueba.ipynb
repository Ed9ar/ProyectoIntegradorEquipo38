{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import pydub\n",
    "import pyaudio\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAKE_WORD_DIRECTORY = \"Audios/HolaTecBot\"\n",
    "BACKGROUND_NOISE_DIRECTORY = \"Audios/HolaCasoNegativo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_data(directory, sample_rate=16000, duration=1):\n",
    "    audio_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if filepath.endswith((\".mp3\", \".ogg\", \".aif\", \".aifc\")):\n",
    "            y, sr = librosa.load(filepath, sr=sample_rate, duration=duration, mono=True)\n",
    "            audio_data.append(y)\n",
    "    return np.array(audio_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    print(\"Creating Model\")\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv1D(64, 3, activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Conv1D(128, 3, activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    print(\"Training\")\n",
    "    X_positive = load_audio_data(WAKE_WORD_DIRECTORY)\n",
    "    X_negative = load_audio_data(BACKGROUND_NOISE_DIRECTORY)\n",
    "    Y_positive = np.ones(len(X_positive))  # Etiquetas para datos positivos\n",
    "    Y_negative = np.zeros(len(X_negative)) # Etiquetas para datos negativos\n",
    "\n",
    "    X = np.concatenate([X_positive, X_negative])\n",
    "    X = np.expand_dims(X, axis=-1)\n",
    "    Y = np.concatenate([Y_positive, Y_negative])\n",
    "\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    Y = Y[indices]\n",
    "\n",
    "    input_shape = X.shape[1:]\n",
    "    model = create_model(input_shape)\n",
    "    model.fit(X, Y, epochs=30, batch_size=32)\n",
    "\n",
    "    model.save(\"wake_word_detection_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_with_microphone():\n",
    "    model = tf.keras.models.load_model('wake_word_detection_model.keras')\n",
    "    audio_stream = None\n",
    "    sample_rate = 44100\n",
    "    duration = 10000\n",
    "    try:\n",
    "        pa = pyaudio.PyAudio()\n",
    "        audio_stream = pa.open(\n",
    "            rate=sample_rate,\n",
    "            channels=1,\n",
    "            format=pyaudio.paInt16,\n",
    "            input=True,\n",
    "            frames_per_buffer=sample_rate *  duration)\n",
    "\n",
    "        print(\"Listening...\")\n",
    "\n",
    "        while True:\n",
    "            print(\"Hola\")\n",
    "            pcm = audio_stream.read(sample_rate * duration)\n",
    "            pcm = np.frombuffer(pcm, dtype=np.int16)\n",
    "\n",
    "            if len(pcm) != sample_rate * duration:\n",
    "                pcm = librosa.resample(pcm.astype(np.float32), len(pcm), sample_rate * duration).astype(np.int16)\n",
    "\n",
    "            pcm = pcm / 32768.0\n",
    "            pcm = np.expand_dims(pcm, axis=0)\n",
    "\n",
    "            prediction = model.predict(pcm)\n",
    "\n",
    "            if prediction > 0.5:\n",
    "                print(\"Wake word detected!\")\n",
    "            else:\n",
    "                print(\"No wake word detected.\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopped listening!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if audio_stream:\n",
    "            audio_stream.stop_stream()\n",
    "            audio_stream.close()\n",
    "        if pa:\n",
    "            pa.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Creating Model\n",
      "Epoch 1/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.8800 - loss: 0.6926\n",
      "Epoch 2/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 569ms/step - accuracy: 0.8400 - loss: 0.4004\n",
      "Epoch 3/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 538ms/step - accuracy: 0.8400 - loss: 0.7210\n",
      "Epoch 4/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 551ms/step - accuracy: 0.8400 - loss: 0.4067\n",
      "Epoch 5/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 516ms/step - accuracy: 0.8400 - loss: 0.4344\n",
      "Epoch 6/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 534ms/step - accuracy: 0.8400 - loss: 0.4576\n",
      "Epoch 7/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 544ms/step - accuracy: 0.8400 - loss: 0.4062\n",
      "Epoch 8/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 547ms/step - accuracy: 0.8400 - loss: 0.3758\n",
      "Epoch 9/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 572ms/step - accuracy: 0.8400 - loss: 0.4118\n",
      "Epoch 10/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 571ms/step - accuracy: 0.8400 - loss: 0.3879\n",
      "Epoch 11/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 555ms/step - accuracy: 0.8400 - loss: 0.3642\n",
      "Epoch 12/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 564ms/step - accuracy: 0.8400 - loss: 0.3618\n",
      "Epoch 13/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 519ms/step - accuracy: 0.8400 - loss: 0.3635\n",
      "Epoch 14/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 608ms/step - accuracy: 0.8400 - loss: 0.3535\n",
      "Epoch 15/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 662ms/step - accuracy: 0.8400 - loss: 0.3344\n",
      "Epoch 16/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 641ms/step - accuracy: 0.8400 - loss: 0.3202\n",
      "Epoch 17/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604ms/step - accuracy: 0.8400 - loss: 0.3124\n",
      "Epoch 18/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 528ms/step - accuracy: 0.8400 - loss: 0.2976\n",
      "Epoch 19/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 517ms/step - accuracy: 0.8400 - loss: 0.2815\n",
      "Epoch 20/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 514ms/step - accuracy: 0.8400 - loss: 0.2687\n",
      "Epoch 21/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 523ms/step - accuracy: 0.8400 - loss: 0.2498\n",
      "Epoch 22/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 524ms/step - accuracy: 0.8400 - loss: 0.2336\n",
      "Epoch 23/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 521ms/step - accuracy: 0.8400 - loss: 0.2210\n",
      "Epoch 24/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 566ms/step - accuracy: 0.8400 - loss: 0.2074\n",
      "Epoch 25/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611ms/step - accuracy: 0.8400 - loss: 0.1932\n",
      "Epoch 26/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 690ms/step - accuracy: 0.8400 - loss: 0.1769\n",
      "Epoch 27/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 679ms/step - accuracy: 0.8400 - loss: 0.1637\n",
      "Epoch 28/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 563ms/step - accuracy: 0.8400 - loss: 0.1543\n",
      "Epoch 29/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 532ms/step - accuracy: 0.8400 - loss: 0.1450\n",
      "Epoch 30/30\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 531ms/step - accuracy: 0.8400 - loss: 0.1351\n"
     ]
    }
   ],
   "source": [
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model_with_microphone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_with_audio_files(test_directory):\n",
    "    model = tf.keras.models.load_model('wake_word_detection_model.keras')\n",
    "    preprocessed_audio_data  = load_audio_data(test_directory)\n",
    "\n",
    "    for i, audio_sample in enumerate(preprocessed_audio_data):\n",
    "        audio_sample = audio_sample.reshape(1, -1)\n",
    "        prediction = model.predict(audio_sample)\n",
    "\n",
    "        if prediction > 0.5:\n",
    "            print(f\"Audio file {i+1}: Wake word detected!\")\n",
    "        else:\n",
    "            print(f\"Audio file {i+1}: No wake word detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step\n",
      "Audio file 1: Wake word detected!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Audio file 2: Wake word detected!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Audio file 3: Wake word detected!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Audio file 4: Wake word detected!\n"
     ]
    }
   ],
   "source": [
    "test_model_with_audio_files('Audios/Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
